{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# KAGGLE COMPETITON AUEB 2021-2022\n## Link Citation Prediction\n##### Link prediction is the problem of predicting the existence of a link between two entities in a network.\n\nThe problem of link prediction has recently attracted a lot of attention in many domains. For instance, in social networks, one may be interested in predicting friendship links among users, while in biology, predicting interactions between genes and proteins is of paramount importance.\n\nIn this challenge, you will deal with the problem of predicting whether a research paper cites another research paper. You are given a citation network consisting of several thousands of research papers, along with their abstracts and their list of authors. The pipeline that is typically followed to deal with the problem is similar to the one applied in any classification problem; the goal is to use edge information to learn the parameters of a classifier and then to use the classifier to predict whether two nodes are linked by an edge or not.\n\nNext, we also provide a simple approach, on which you can work. The key component is how we create our training and test data. For training, you may first create a numpy array of zeros with 2m rows and n columns (where m is the number of edges in the edgelist file, and n the number of desired features). We then need to populate this array to create the training features. A way to do that is to iterate over all edges in the edgelist file, and if we want to have 3 features: we get the degree of the first node of the edge as the 1st column, the degree of the 2nd node of the edge as the 2nd column and their sum as the 3rd column. For the remaining m rows, we select randomly nodes and get their degrees and sum as features as well. Now we have our matrix for training. We also need to create the y classes. We create a vector of zeros with size 2m. The first m elements represent the edges that are present in the edgelist. Thus we make them 1. The remaining need to be 0, as they represent non-existing edges. We are now ready to give the features and classes to a scikitlearn classifier via the 'fit' method. Afterwards, we only need to create a similar feature matrix for the test file. We then need the probabilities of our predictions. Remember to get the probabilities only for the positive class. Please, check the 'submissionsrandom' file to check if the format matches your output. The 1st column in the submissions file represents an id integer of the edges in test file. 0 points to the first edge of the test file and so on. You are now ready to submit!","metadata":{}},{"cell_type":"markdown","source":"\t---\n## Import Statements","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport pandas as pd\nimport numpy as np\nfrom random import randint\nfrom collections import Counter\nfrom itertools import combinations\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-05-14T19:41:45.261271Z","iopub.execute_input":"2022-05-14T19:41:45.262217Z","iopub.status.idle":"2022-05-14T19:41:47.088372Z","shell.execute_reply.started":"2022-05-14T19:41:45.262159Z","shell.execute_reply":"2022-05-14T19:41:47.087399Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"\t---\n## Data Acquisition\n#### Read Edgelist","metadata":{}},{"cell_type":"code","source":"# Create a graph\nG = nx.read_edgelist('../input/linkCitatationData/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\nnodes = list(G.nodes())\n\nnum_of_nodes = G.number_of_nodes()\nnum_of_edges = G.number_of_edges()\n\nprint('Number of nodes:', num_of_nodes)\nprint('Number of edges:', num_of_edges)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Read Abstracts","metadata":{}},{"cell_type":"code","source":"# Read the abstract of each paper\nabstracts = dict()\n\nwith open('../input/linkCitatationData/abstracts.txt', 'r', encoding = \"UTF-8\") as f:\n    for line in f:\n        node, abstract = line.split('|--|')\n        \n        abstracts[int(node)] = abstract\n\nfor node in abstracts:\n    abstracts[node] = set(abstracts[node].split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Read Authors","metadata":{}},{"cell_type":"code","source":"authors = dict()\nwith open('../input/linkCitatationData/authors.txt', 'r', encoding = \"UTF-8\") as f:\n    for line in f:\n        node, author = line.split('|--|')\n\n        author = author.replace(\"\\n\", \"\")\n        \n        authors[int(node)] = author.split(\",\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\t---\n## Data Preparation\n#### Baseline Data","metadata":{}},{"cell_type":"code","source":"# Features:\n# (1) sum of number of unique terms of the two nodes' abstracts\n# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n# (3) number of common terms between the abstracts of the two nodes\n\nx = np.zeros((2*num_of_edges, 3))\ny = np.zeros(2*num_of_edges)\n\nfor i, edge in enumerate(G.edges()):\n    # an edge\n    x[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n    x[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n    x[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n\n    y[i] = 1\n\n    # a randomly generated pair of nodes\n    n1 = randint(0, num_of_nodes-1)\n    n2 = randint(0, num_of_nodes-1)\n    x[num_of_edges+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n    x[num_of_edges+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n    x[num_of_edges+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n    \n    y[num_of_edges+i] = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Baseline Data + Common Authors","metadata":{}},{"cell_type":"code","source":"# Features:\n# (1) sum of number of unique terms of the two nodes' abstracts\n# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n# (3) number of common terms between the abstracts of the two nodes\n# (4) number of common Authors between the Author List of the two nodes\n\nx = np.zeros((2*num_of_edges, 4))\ny = np.zeros(2*num_of_edges)\n\nfor i, edge in enumerate(G.edges()):\n    # an edge\n    x[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n    x[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n    x[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n    x[i,3] = len(set(authors[edge[0]]).intersection(authors[edge[1]]))\n\n    y[i] = 1\n\n    # a randomly generated pair of nodes\n    n1 = randint(0, num_of_nodes-1)\n    n2 = randint(0, num_of_nodes-1)\n    x[num_of_edges+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n    x[num_of_edges+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n    x[num_of_edges+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n    x[num_of_edges+i,3] = len(set(authors[n1]).intersection(authors[n2]))\n    \n    y[num_of_edges+i] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Baseline Data - Sum of Unique Terms + Common Authors\n","metadata":{}},{"cell_type":"code","source":"# Features:\n# (1) absolute value of difference of number of unique terms of the two nodes' abstracts\n# (2) number of common terms between the abstracts of the two nodes\n# (3) number of common Authors between the Author List of the two nodes\n\nx = np.zeros((2*num_of_edges, 3))\ny = np.zeros(2*num_of_edges)\n\nfor i, edge in enumerate(G.edges()):\n    # an edge\n    x[i,0] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n    x[i,1] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n    x[i,2] = len(set(authors[edge[0]]).intersection(authors[edge[1]]))\n\n    y[i] = 1\n\n    # a randomly generated pair of nodes\n    n1 = randint(0, num_of_nodes-1)\n    n2 = randint(0, num_of_nodes-1)\n    x[num_of_edges+i,0] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n    x[num_of_edges+i,1] = len(abstracts[n1].intersection(abstracts[n2]))\n    x[num_of_edges+i,2] = len(set(authors[n1]).intersection(authors[n2]))\n    \n    y[num_of_edges+i] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Papers","metadata":{}},{"cell_type":"code","source":"paper_times_cited = Counter([edge[0] for edge in G.edges])\npaper_num_of_citations = Counter([edge[1] for edge in G.edges])\n\n# Features:\n# (1) probability of getting cited for the first paper\n# (2) probability of citing a paper for the second paper\n\nx = np.zeros((2*num_of_edges, 4))\ny = np.zeros(2*num_of_edges)\n\nfor i, edge in enumerate(G.edges()):\n    # an edge\n    \n    x[i,0] = paper_times_cited[edge[0]]/num_of_edges\n\n    x[i,1] = paper_num_of_citations[edge[1]]/num_of_edges\n\n    y[i] = 1\n\n    # a randomly generated pair of nodes\n    n1 = randint(0, num_of_nodes-1)\n    n2 = randint(0, num_of_nodes-1)\n\n    x[num_of_edges+i,0] = paper_times_cited[n1]/num_of_edges\n\n    x[num_of_edges+i,1] = paper_num_of_citations[n2]/num_of_edges\n    \n    y[num_of_edges+i] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Authors","metadata":{}},{"cell_type":"code","source":"#distinct_authors = list(set([author for author_list in authors.values() for author in author_list]))\n\ntimes_cited = Counter([author for edge in G.edges for author in authors[edge[0]]])\nnum_of_citations = Counter([author for edge in G.edges for author in authors[edge[1]]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features:\n# (1) probability of getting cited for the first paper\n# (2) probability of citing a paper for the second paper\n# (3) number of common terms between the abstracts of the two nodes\n# (4) number of common Authors between the Author List of the two nodes\n\nx = np.zeros((2*num_of_edges, 4))\ny = np.zeros(2*num_of_edges)\n\nfor i, edge in enumerate(G.edges()):\n    # an edge\n    \n    x[i,0] = 1\n    for author in authors[edge[0]]:\n        x[i,0] *= times_cited[author]/num_of_edges\n\n    x[i,1] = 1\n    for author in authors[edge[1]]:\n        x[i,1] *= num_of_citations[author]/num_of_edges\n\n    x[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n    x[i,3] = len(set(authors[edge[0]]).intersection(authors[edge[1]]))\n\n    y[i] = 1\n\n    # a randomly generated pair of nodes\n    n1 = randint(0, num_of_nodes-1)\n    n2 = randint(0, num_of_nodes-1)\n\n    x[num_of_edges+i,0] = 1\n    for author in authors[n1]:\n        x[num_of_edges+i,0] *= times_cited[author]/num_of_edges\n\n    x[num_of_edges+i,1] = 1\n    for author in authors[n2]:\n        x[num_of_edges+i,1] *= num_of_citations[author]/num_of_edges\n\n    x[num_of_edges+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n    x[num_of_edges+i,3] = len(set(authors[n1]).intersection(authors[n2]))\n    \n    y[num_of_edges+i] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\t---\n#### Shuffle Data and Split into Train/Test","metadata":{}},{"cell_type":"code","source":"x, y = shuffle(x, y)\nx_train, x_test, y_train, y_test = train_test_split(x, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\t---\n## Models\n##### Gaussian Naive Bayes","metadata":{}},{"cell_type":"code","source":"gnb = GaussianNB()\ny_pred = gnb.fit(x_train, y_train).predict(x_test)\n\nprint(\"Accuracy: \" + str(accuracy_score(y_pred, y_test)) + \"%\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Use logistic regression to predict if two nodes are linked by an edge\nclf = LogisticRegression(solver='liblinear',random_state=34)\nclf.fit(x_train, y_train)\ny_pred = clf.predict_proba(x_test)\ny_pred = y_pred[:,1]\n\ngetAccuracy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Linear Regression ","metadata":{}},{"cell_type":"code","source":"clf = LinearRegression()\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\n\ngetAccuracy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Ada-Boost","metadata":{}},{"cell_type":"code","source":"abc = AdaBoostClassifier(n_estimators=10, learning_rate=1)\n\nmodel = abc.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\ngetAccuracy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Voting Regressor","metadata":{}},{"cell_type":"code","source":"r1 = LinearRegression()\nr2 = RandomForestRegressor(n_estimators=10, random_state=1)\nr3 = KNeighborsRegressor()\ner = VotingRegressor([('lr', r1), ('rf', r2), ('r3', r3)])\n\nmodel = er.fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\ngetAccuracy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\t---\n## Metrics\n#### Accuracy Score","metadata":{}},{"cell_type":"code","source":"def getAccuracy():\n    y_predictions = []\n    for i in y_pred: \n        if i >= 0.5: \n            y_predictions.append(1)\n        else:\n            y_predictions.append(0)\n\n\n    print(\"Accuracy: \" + str(accuracy_score(y_predictions, y_test)) + \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\t---\n## Accuracy Score\n\n#### Baseline Data:\n\n\tGaussian Naive Bayes: 0.660876445571067%\n\n\tLogistic Regression:\n\t\tRS 34: 0.7141752964405159% (Baseline)\n\t\tRS 54: 0.7129389828894205%\n\n\tLinear Regression: 0.7173146170724828%\n\n\tAda-Boost:\n\t\t10 E, 1 lR: 0.7015484140386609%\n\t\t50 E, 1 lR: 0.7171332910849888%\n\t\t10 E, 5 lR: 0.3140895787009733%\n\t\t50 E, 5 lR: 0.3140895787009733%\n\n#### Baseline Data + Common Authors:\n\n\tGaussian Naive Bayes: 0.5558905303876713%\n\n\tLogistic Regression:\n\t\tRS 34: 0.7238331947441106% (0.7260475696822949% with \\n removed)\n\t\tRS 54: 0.7226939547014715%\n\n\tLinear Regression: 0.7215931777470888% (0.7231866485462785% with \\n removed)\n\n\tAda-Boost:\n\t\t10 E, 1 lR: 0.7105487766906359%\n\t\t50 E, 1 lR: 0.7245456776646678%\n\t\t10 E, 5 lR: 0.3146756829029741%\n\t\t50 E, 5 lR: 0.3146756829029741%\n\n\t\t100 E, 1 lR: 0.7262582008798889%","metadata":{}},{"cell_type":"markdown","source":"## Ideas\n\nCreate Friendly  Connections - Relations between Authors, and produce its predictions\n\n","metadata":{}},{"cell_type":"markdown","source":"## Dataframe","metadata":{}},{"cell_type":"markdown","source":"#### Reading Files","metadata":{}},{"cell_type":"code","source":"# Read the abstract of each paper\nabstracts = [] #Init List Abstracts\n\nwith open('../input/linkCitatationData/abstracts.txt', 'r', encoding = \"UTF-8\") as f:\n    for line in f:\n        abstracts.append(line.split('|--|')[1].replace(\"\\n\", \"\"))  \n        \nauthors = [] #Init List Authors\nwith open('../input/linkCitatationData/authors.txt', 'r', encoding = \"UTF-8\") as f:\n    for line in f:\n        authors.append(line.split('|--|')[1].replace(\"\\n\", \"\").split(\",\")) \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Dataframe","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(data = {'abstracts': abstracts, 'authors': authors})\nprint(df.iloc[0,1][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a graph\n\nG = nx.read_edgelist('../input/linkCitatationData/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\nnodes = list(G.nodes())\n\nnum_of_nodes = G.number_of_nodes()\nnum_of_edges = G.number_of_edges()\nadj = nx.adjacency_matrix(G)\nprint('Number of nodes:', num_of_nodes)\nprint('Number of edges:', num_of_edges)\n#print('Adjacency Matrix:', adj[: 5][: 5])\nprint(adj[0][1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(num_of_nodes):\n    for j in range(num_of_nodes):\n        ","metadata":{},"execution_count":null,"outputs":[]}]}