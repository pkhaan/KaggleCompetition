{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word and Document Embeddings\n",
    "\n",
    "In this part of the lab, we will play with word and document embeddings and see how they can be applied to (un)supervised document classification. We will make use of the scikit-learn and gensim Python libraries. Finally, we will also need the \"Google News\" pre-trained word vectors. If not already done, you can download them from  here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing. This binary file contains embeddings for more than 3 million unique words and phrases, learned with word2vec from a corpus of more than 100 billion words (see [Mikolov et al. NIPS'13](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) for more details).\n",
    "\n",
    "Throughout the entire session, we will be working with the BBC Sport dataset (available at http://mlg.ucd.ie/datasets/bbc.html). The dataset consists of sports news articles from the BBC Sport website. There are 737 articles in total categorized into the following 5 classes: (1) athletics, (2) cricket, (3) football, (4) rugby, and (5) tennis. Let's first read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    labels = []\n",
    "    docs =[]\n",
    "\n",
    "    with open(filename, encoding='utf8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            content = line.split('\\t')\n",
    "            labels.append(content[0])\n",
    "            docs.append(content[1][:-1])\n",
    "    \n",
    "    return docs,labels\n",
    "\n",
    "docs, class_labels = load_data('data/bbcsport.txt')\n",
    "print(\"Example of an article:\", docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents that are contained in the dataset have already undergone some preprocessing. We will apply some further preprocessing steps. More specifically, we will remove some punctuation marks, diacritics, and non letters, if any. Furthermore, we will represent each document as a list of tokens. We will also randomly shuffle the articles, and finally, since the class labels are strings, we will encode them with values between 0 and n_classes-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().split()\n",
    "\n",
    "    \n",
    "def preprocessing(docs):\n",
    "    preprocessed_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        preprocessed_docs.append(clean_str(doc))\n",
    "\n",
    "    return preprocessed_docs\n",
    "\n",
    "processed_docs = preprocessing(docs)\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(class_labels)\n",
    "\n",
    "n = len(processed_docs)\n",
    "idx = np.random.permutation(n)\n",
    "\n",
    "processed_docs_rand = list()\n",
    "y = np.zeros(n, dtype=np.int64)\n",
    "for i in range(idx.size):\n",
    "    processed_docs_rand.append(processed_docs[idx[i]])\n",
    "    y[i] = labels[idx[i]]\n",
    "\n",
    "processed_docs = processed_docs_rand\n",
    "\n",
    "print(\"Preprocessed document:\", processed_docs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with word embeddings \n",
    "First, we will get familiar with some properties of word embeddings by performing some operations manually.\n",
    "\n",
    "We will first make use of gensimâ€™s build_vocab() method which extracts the vocabulary from the list of documents (where each document is a list of tokens). Create a new list of documents. Insert all existing documents to the list. Furthermore, add another document consisting of the following 8 tokens: 'queen', 'king', 'woman', 'man', 'aunt', 'uncle', 'son', 'daughter'. Then, load the Google News word vectors corresponding to our vocabulary words. This is a trick to avoid having to load all the vectors into memory (it would require 5-6 GB of RAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# returns cosine similarity between two vectors\n",
    "def cosine(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))\n",
    "\n",
    "# returns the vector of a word\n",
    "def my_vector_getter(word, model):\n",
    "    try:\n",
    "        word_array = model.wv[word].astype(np.float64)\n",
    "        return word_array\n",
    "    except KeyError:\n",
    "        print('word: <', word, '> not in vocabulary!')\n",
    "\n",
    "# returns cosine similarity between two word vectors\n",
    "def my_cos_similarity(word1, word2, model):\n",
    "    return cosine(my_vector_getter(word1, model), my_vector_getter(word2, model))\n",
    "\n",
    "model = Word2Vec(size=300, min_count=0)\n",
    "\n",
    "\n",
    "#your code here\n",
    "\n",
    "\n",
    "# load vectors corresponding to our vocabulary\n",
    "path_to_embeddings = '...'\n",
    "model.intersect_word2vec_format(path_to_embeddings, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cosine similarity in the embedding space between semantically close words (e.g., \"man\" and \"woman\") and between unrelated words. You can make use of the functions defined above. What do you observe? Similarly, perform some vector operations (e.g., \"king\"-\"man\"+\"woman\") and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Project the word vectors into a lower-dimensional space using PCA/t-SNE and visualize the projections of the 200 most frequent words. What can you say about the embedding space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ========== visualize word embeddings of 'n_mf' most frequent words ==========\n",
    "\n",
    "n_mf = 200\n",
    "all_tokens = [token for sublist in processed_docs for token in sublist]\n",
    "t_counts = dict(Counter(all_tokens))\n",
    "sorted_t_counts = sorted(t_counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "mft = [elt[0] for elt in sorted_t_counts]\n",
    "\n",
    "# store the vectors of the most frequent words in numpy array\n",
    "mft_vecs = np.zeros((n_mf,300))\n",
    "for i,token in enumerate(mft[:n_mf]):\n",
    "    mft_vecs[i,:] = model.wv[token]\n",
    "\n",
    "my_pca = PCA(n_components=10)\n",
    "my_tsne = TSNE(n_components=2)\n",
    "\n",
    "mft_vecs_pca = my_pca.fit_transform(mft_vecs)\n",
    "mft_vecs_tsne = my_tsne.fit_transform(mft_vecs_pca)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(mft_vecs_tsne[:,0], mft_vecs_tsne[:,1],s=3)\n",
    "for xx, yy, token in zip(mft_vecs_tsne[:,0] , mft_vecs_tsne[:,1], mft):     \n",
    "    ax.annotate(token, xy=(xx, yy), size=8)\n",
    "fig.suptitle('t-SNE visualization of word embeddings',fontsize=20)\n",
    "fig.set_size_inches(11,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, observe some regularities in the space made of the first two PCs (e.g., gender regularities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ========== visualize regularities among word vectors ==========\n",
    "\n",
    "my_pca = PCA(n_components=2)\n",
    "# numpy array containg vectors of all words\n",
    "all_vecs = model.wv.vectors\n",
    "all_vecs_pca = my_pca.fit_transform(all_vecs) \n",
    "\n",
    "my_words = ['queen','king','woman','man','aunt','uncle','son','daughter']\n",
    "\n",
    "# w2v.wv.index2word contains the words in the order in which they appear in w2v.wv.syn0\n",
    "idxs = [model.wv.index2word.index(elt) for elt in my_words]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(all_vecs_pca[idxs,0], all_vecs_pca[idxs,1],s=3)\n",
    "for xx, yy, token in zip(all_vecs_pca[idxs,0], all_vecs_pca[idxs,1], my_words):     \n",
    "    ax.annotate(token, xy=(xx, yy), size=8)\n",
    "fig.suptitle('PCA visualization of gender regularities',fontsize=15)\n",
    "fig.set_size_inches(7,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document embeddings for supervised text categorization\n",
    "We will next use an unsupervised method that generates document representations and we will apply it for classifying the articles from the BBC Sport dataset. We will first split the dataset into a training and a test set. Use 90% of the documents for training. The remaining documents will serve as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will experiment with doc2vec ([Le and Mikolov ICML'14](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)). Doc2vec is an extension of word2vec that can learn vectors for all the documents in a collection in a fully unsupervised manner. The embeddings can thus be used for unsupervised or supervised classification. In a supervised setting, an inference stage is required to obtain the vectors of the documents in the test set. The model is simply trained on the new documents, with all parameters fixed. Before using doc2vec features with a SVM for supervised classification, we will learn document embeddings for our training set with the PV-DBOW architecture (using gensim's [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html) object). Are the similarities between documents meaningful? Do similar documents share the same labels? Visualize 2D maps of the document embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "\n",
    "d2v_training_data = []\n",
    "for i,doc in enumerate(processed_docs_train):\n",
    "    d2v_training_data.append(TaggedDocument(words=doc,tags=[i]))\n",
    "    \n",
    "# ========== learning doc embeddings with doc2vec ==========\n",
    "\n",
    "# PV stands for 'Paragraph Vector'\n",
    "# PV-DBOW (distributed bag-of-words) dm=0\n",
    "\n",
    "d2v = Doc2Vec(d2v_training_data, vector_size=100, window=10, alpha=0.1, min_alpha=1e-4, dm=0, negative=1, epochs=10, min_count=2, workers=4)\n",
    "d2v_vecs = np.zeros((len(processed_docs_train), 100))\n",
    "for i in range(len(processed_docs_train)):\n",
    "    d2v_vecs[i,:] = d2v.docvecs[i]\n",
    "\n",
    "# ========== experimenting with doc2vec ==========\n",
    "\n",
    "print(d2v.docvecs.most_similar(0))\n",
    "idxs_most_similar = [elt[0] for elt in d2v.docvecs.most_similar(0)]\n",
    "print([y_train[idx] for idx in idxs_most_similar])\n",
    "\n",
    "# visualize document embeddings\n",
    "n_plot = 1000\n",
    "\n",
    "my_pca = PCA(n_components=10)\n",
    "my_tsne = TSNE(n_components=2)\n",
    "d2v_vecs_pca = my_pca.fit_transform(d2v_vecs[:n_plot,:]) \n",
    "d2v_vecs_tsne = my_tsne.fit_transform(d2v_vecs_pca)\n",
    "\n",
    "labels_plt = list(y_train[:n_plot])\n",
    "\n",
    "palette = plt.get_cmap('hsv',len(list(set(labels_plt))))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "my_colors = {0:'r', 1:'b', 2: 'g', 3:'y', 4:'k'}\n",
    "\n",
    "for label in list(set(labels_plt)):\n",
    "    idxs = [idx for idx,elt in enumerate(labels_plt) if elt==label]\n",
    "    ax.scatter(d2v_vecs_tsne[idxs,0], \n",
    "               d2v_vecs_tsne[idxs,1], \n",
    "               c = my_colors[label],\n",
    "               label=str(label),\n",
    "               alpha=0.7,\n",
    "               s=40)\n",
    "\n",
    "ax.legend(scatterpoints=1)\n",
    "fig.suptitle('t-SNE visualization of document embeddings',fontsize=20)\n",
    "fig.set_size_inches(11,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare doc2vec against the traditional bag-of-words representation with tf-idf weighting in the task of text categorization. Use the [infer_vector](https://radimrehurek.com/gensim/models/doc2vec.html) method of doc2vec to generate representations for the documents of the test set. Use the [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) object of scikit-learn to generate the traditional bag-of-words representation and the [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) object to perform text categorization. To calculate the accuracies of the two approaches, use the [accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function of scikit-learn. What do you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
