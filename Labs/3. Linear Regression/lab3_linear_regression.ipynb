{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Linear Regression\n",
    "\n",
    "In this lab, we will study linear regression. Specifically, we will fit a linear regression model to a synthetic dataset and we will also apply it to the well-known Boston Housing dataset.\n",
    "\n",
    "Linear regression is an approach for modeling the relationship between a set of independent variables $X$ (also known as features, attributes, predictors) and a dependent variable $y$. This method assumes the relationship between each feature $x \\in X$ is linearly related to the dependent variable $y$. In general, both the features and the dependent variable are assumed to be continuous.\n",
    "\n",
    "The first part of this lab is to create a toy dataset in order to illustrate how linear regression works. The dataset will consist of $100$ points in the $2$-dimensional space ($n = 100$, $d = 2$). \n",
    "\n",
    "Subsequently, we will use linear regression to learn a model that given the first dimension of a point is capable of predicting its second dimension. More specifically, the first dimension of each point will be an integer number between $1$ and $100$ ($x_1 = 1$ for the first point, $x_2 = 2$ for the second point, etc). The second dimension will be equal to the first plus some Gaussian noise. To generate these values make use of the `normal` function (https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html) that returns a sample from the 'standard normal' distribution as follows:\n",
    "\n",
    "$$y_i = x_i + np.random.normal(...)$$\n",
    "\n",
    "Set the mean equal to $0$ and the standard deviation equal to $10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate the data set  \n",
    "X = np.arange(100.0).reshape((100, 1))\n",
    "noise = np.random.normal(0,10,(100,1))\n",
    "Y = X + noise\n",
    "\n",
    "# Save old values\n",
    "X_Old = X\n",
    "Y_Old = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the generated data in a $2$-dimensional plane using `scatter` (http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot the data\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a `supervised learning` problem, we are given a training set $D = \\{{x}_i,y_i\\}_{i=1}^n$ and our goal is to learn a model to make predictions on previously unseen data. Consider a learning machine as a function $f(\\mathbf{x}|\\boldsymbol{\\theta})$ mapping an input $\\mathbf{x}$ to an output $\\hat{y}$. In such a setting, we aim to learn the parameters $\\boldsymbol{\\theta}$ from the training data. After learning $\\boldsymbol{\\theta}$, given a new value of the input $\\mathbf{x}_{n+1}$, we can use $f$ to make a prediction $\\hat{y}_{n+1}$\n",
    "\n",
    "In linear regression, the function $f$ is a linear combination of the inputs (the entries of vector $\\mathbf{x}$)\n",
    "$$\\hat{y}_i = f(\\mathbf{x}_i|\\boldsymbol{\\theta}) = \\sum_{j=0}^d x_{ij} \\theta_j = \\mathbf{x}_i^T \\boldsymbol{\\theta} = \\theta_0 \\cdot x_{i0} + \\theta_1 \\cdot x_{i1} + \\theta_2 \\cdot x_{i2} + \\ldots + \\theta_d \\cdot x_{id}$$\n",
    "where we have assumed that $x_{i0} = 1$\n",
    "\n",
    "For more than one examples ($n$ examples), the above expression becomes:\n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{X} \\boldsymbol{\\theta}$$\n",
    "\n",
    "with $\\mathbf{\\hat{y}} \\in \\mathbb{R}^{n \\times 1}$, $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and $\\boldsymbol{\\theta} \\in \\mathbb{R}^{d \\times 1}$. \n",
    "\n",
    "That is,\n",
    "$$\n",
    "      \t\\begin{bmatrix}\n",
    "       \t  \\hat{y}_i \\\\[0.3em]\n",
    "       \t  \\vdots \\\\[0.3em]\n",
    "       \t  \\hat{y}_n\n",
    "     \t\\end{bmatrix}\n",
    "     \t=\n",
    "      \t\\begin{bmatrix}\n",
    "       \t  1 & x_{11} & \\ldots & x_{1d} \\\\[0.3em]\n",
    "       \t  \\vdots & \\vdots & \\vdots & \\vdots \\\\[0.3em]\n",
    "       \t  1 & x_{n1} & \\ldots & x_{nd}\n",
    "     \t\\end{bmatrix}\n",
    "     \t\\begin{bmatrix}\n",
    "       \t  \\theta_0 \\\\[0.3em]\n",
    "       \t  \\vdots \\\\[0.3em]\n",
    "       \t  \\theta_d\n",
    "     \t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Your next task is to perform linear regression to fit a line to the generated data. Use the function `LinearRegression` provided by `scikit-learn` (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Use the predictions of linear regression for the training data to plot the straight line that is fitted to it. As you will see later, linear regression attempts to draw a straight line that minimizes the residual sum of squares between the predicted and the observed values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we employed the `LinearRegression` function provided by `scikit-learn`. You will next write your own algorithm for performing linear regression.\n",
    "\n",
    "Before delving into the details of linear regression, we will standardize our data. Feature standardization makes the values of each feature have zero-mean and unit-variance. This method is widely used for normalization in many machine learning algorithms (linear regression, logistic regression, neural networks etc). First we compute the mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values of each feature by its standard deviation:\n",
    "\n",
    "$$\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}$$\n",
    "$$\\sigma_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\mu_j)^2}$$\n",
    "$$x_{ij} \\leftarrow \\frac{x_{ij} - \\mu_j}{\\sigma_j}$$\n",
    "\n",
    "Write a function that takes as input a data matrix $X$ and returns a normalized version of $X$ as well as the mean value and the standard deviation of each feature. Use the equations given above. Run the function to get the standardized data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "def featureNormalize(X):\n",
    "    # Returns a normalized version of X where\n",
    "    # the mean value of each feature is 0 and the standard deviation\n",
    "    # is 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned above, for each training example, we assume that $x_{i0} = 1$. This corresponds to the intercept of the linear model. It is very useful to add these values to the training matrix $\\mathbf{X}$. Add an extra column to matrix $\\mathbf{X}$ whose entries are all equal to $1$ to accommodate the intercept term. This should correspond to the first column of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "# Add extra column with ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression minimizes the squared error on the training data:\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\big(y_i - f(\\mathbf{x}_i|\\boldsymbol{\\theta})\\big)^2 = \\sum_{i=1}^n \\big(y_i - \\mathbf{x}_i \\boldsymbol{\\theta})\\big)^2 = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta})^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta})$$\n",
    "\n",
    "If we do well on the training examples, we should surely do well on unseen examples.\n",
    "\n",
    "Write a function that takes as input a training matrix $\\mathbf{X}$, its true outputs $\\mathbf{y}$ and the parameters $\\boldsymbol{\\theta}$, and returns the squared error on the training data. Use the equation given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "from math import pow\n",
    "\n",
    "def computeCost(X, y, theta):\n",
    "    # Computes the cost of using theta as the parameter for\n",
    "    # linear regression to fit the data points in X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the squared error on the training data, we will employ ``gradient descent``. Gradient descent is an optimization method that starts with some “initial guess” for $\\boldsymbol{\\theta}$, and repeatedly changes $\\boldsymbol{\\theta}$ to make $J(\\boldsymbol{\\theta})$ smaller. More specifically, gradient descent repeatedly performs the update:\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_j}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. This is a very natural algorithm that repeatedly takes a step in the direction of steepest decrease of $J$.\n",
    "\n",
    "\n",
    "For the case of one training example $D = \\{\\mathbf{x},y\\}$, the partial derivative is:\n",
    "\n",
    "$$\\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} (y - \\mathbf{x}^T\\boldsymbol{\\theta})^2 = 2(y - \\mathbf{x}^T \\boldsymbol{\\theta}) \\frac{\\partial}{\\partial \\theta_j} (y - \\mathbf{x}^T \\boldsymbol{\\theta}) = 2(y - \\mathbf{x}^T \\boldsymbol{\\theta}) \\frac{\\partial}{\\partial \\theta_j} \\Big( \\sum_{i=0}^n y - x_i \\theta_i \\Big) = -2(y - \\mathbf{x}^T \\boldsymbol{\\theta}) x_j$$\n",
    "\n",
    "Hence, for a single training example, this gives the update rule:\n",
    "\n",
    "$$\\theta_j = \\theta_j + 2 \\alpha (y - \\mathbf{x}^T \\boldsymbol{\\theta}) x_j$$\n",
    "\n",
    "There are two main common ways of performing gradient descent:\n",
    "1. ``Batch gradient descent``: look at every example in the entire training set on every step and then update the parameters $\\boldsymbol{\\theta}$\n",
    "    \n",
    "        While not converged\n",
    "            For all j      \n",
    "$\\qquad \\qquad \\qquad \\theta_j = \\theta_j + 2 \\alpha \\sum_{i=1}^n(y_i - \\mathbf{x_i}^T \\boldsymbol{\\theta}) x_{ij}$    \n",
    "            End For\t\n",
    "        End While\n",
    "\n",
    "2. ``Stochastic gradient descent``: repeatedly run through the training set and update the parameters $\\boldsymbol{\\theta}$ each time we encounter a training example\n",
    "    \n",
    "        While not converged\n",
    "            For i=1 to n\n",
    "                For all j\n",
    "$\\qquad \\qquad \\qquad \\theta_j = \\theta_j + 2 \\alpha (y_i - \\mathbf{x_i}^T \\boldsymbol{\\theta}) x_{ij}$\n",
    "                End For\n",
    "            End For\t\n",
    "        End While\n",
    "        \n",
    "Write a function that takes as input a training matrix $\\mathbf{X}$, its true outputs $\\mathbf{y}$, the initial parameters $\\boldsymbol{\\theta}$, the learning rate $\\alpha$ and the maximum number of iterations $num\\_iters$, and performs batch gradient descent in order to minimize  the squared error on the training data. Use the equation given above. Perform at most $num\\_iters$ iterations. Return the parameters $\\boldsymbol{\\theta}$ and a vector that contains the squared error at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    # Performs gradient descent and updates theta by\n",
    "    # taking num_iters gradient steps with learning rate alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform linear regression on the generated dataset using your own algorithm. Set the learning rate $\\alpha$ equal to $0.0005$ and the maximum number of iterations equal to $100$. Initialize parameters $\\boldsymbol{\\theta}$ to random values sampled from the “standard normal” distribution. Show the convergence of the algorithm by plotting the squared error at each iteration. Compute the error at each iteration and return a vector that contains these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code to plot the generated data and the line learned by linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data and line returned by Gradient Descent\n",
    "points = np.zeros((2,2))\n",
    "points[0,0] = 0\n",
    "points[0,1] = (1 * theta[0]) + (((points[0,0]-mu)/sigma)*theta[1])\n",
    "points[1,0] = 100\n",
    "points[1,1] = (1 * theta[0]) + (((points[1,0]-mu)/sigma)*theta[1])\n",
    "plt.scatter(X_Old,Y_Old)\n",
    "plt.plot(points[:,0], points[:,1], color='red', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute parameters $\\boldsymbol{\\theta}$ in closed form as shown below. The squared error function can be written  as:\n",
    "\n",
    "$$J(\\boldsymbol{\\theta}) = (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta})^T (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}) = (\\mathbf{y}^T - \\boldsymbol{\\theta}^T \\mathbf{X}^T) (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}) = \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^T \\mathbf{X} \\boldsymbol{\\theta} - \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{y} + \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} = \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{y}^T \\mathbf{X} \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta}$$\n",
    "\n",
    "We will need the following results from matrix differentiation:\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{A} \\boldsymbol{\\theta}}{\\partial \\boldsymbol{\\theta}} = \\mathbf{A}^T \\text{ and } \\frac{\\partial \\boldsymbol{\\theta}^T \\mathbf{A} \\boldsymbol{\\theta}}{\\partial \\boldsymbol{\\theta}} = 2 \\mathbf{A}^T \\boldsymbol{\\theta}$$\n",
    "    \n",
    "By setting the derivative equal to zero, we have:\n",
    "\n",
    "$$\\frac{\\partial J(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = 0 \\Rightarrow \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} (\\mathbf{y}^T\\mathbf{y} - 2\\mathbf{y}^T \\mathbf{X} \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta}) = 0 \\Rightarrow 0 - 2\\mathbf{X}^T \\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X} \\boldsymbol{\\theta} = 0 \\Rightarrow \\mathbf{X}^T\\mathbf{X} \\boldsymbol{\\theta} = \\mathbf{X}^T \\mathbf{y}  \\Rightarrow \\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "Use the above equation to compute parameters $\\boldsymbol{\\theta}$ and plot the generated data and the line learned by linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the line fitted to the data same as the one fitted in the case of gradient descent?\n",
    "\n",
    "---\n",
    "\n",
    "We will next perform linear regression on the Boston Housing dataset (https://archive.ics.uci.edu/ml/machine-learning-databases/housing/) that is available from `scikit-learn` (http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). The Boston Housing dataset contains information about the housing values in suburbs of Boston. This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University and is now available on the UCI Machine Learning Repository.\n",
    "\n",
    "The datasets consists of $506$ samples and each sample is described by $13$ features such as\n",
    "<ul>\n",
    "  <li>per capita crime rate by town</li>\n",
    "  <li>nitric oxides concentration (parts per 10 million)</li>\n",
    "  <li>average number of rooms per dwelling</li>\n",
    "  <li>weighted distances to five Boston employment centres</li>\n",
    "</ul>\n",
    "\n",
    "Your first task is to load the dataset and the class labels. Follow the link given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded dataset is a dictionary-like structure with the following keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print boston.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to get a description of the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print boston.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset itself corresponds to the value of the dictionary keyed by the string 'data', while the prices to the value keyed by the string 'target'. Run the following code to assign the dataset to a variable $X$ and the prices to a variable $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to split the dataset into a training and a test set. To do this, use the function `train_test_split` of `scikit-learn` (http://scikit-learn.org/0.16/modules/generated/sklearn.cross_validation.train_test_split.html). Set the test size equal to $\\frac{2}{10}$ of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use linear regression and learn the linear model that best describes the training data. Use the learned model to predict the prices of the houses in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the squared error for the houses of the test set. Use a larger size for the test set and perform once again linear regression. Compute the squared error for the houses of the new test set. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
