{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from scipy import spatial\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.clf = xgb.XGBClassifier()\n",
    "        #self.clf = svm.SVC(kernel='linear', gamma=2)\n",
    "    def fit(self, X, y):\n",
    "        print ('fit classifier...')\n",
    "        self.clf.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        print ('predict classifier...')\n",
    "        return self.clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (746092751.py, line 89)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [7]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print i\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import igraph\n",
    "from library import read_files, clean_text_simple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from networkx.algorithms.connectivity import local_edge_connectivity\n",
    "from networkx.algorithms.connectivity import(build_auxiliary_edge_connectivity)\n",
    "from networkx.algorithms.flow import build_residual_network\n",
    "#data_path = '../data/'\n",
    "#model = gensim.models.word2vec.Word2Vec.load(data_path + 'custom_w2v_model.txt')\n",
    "#model.intersect_word2vec_format(data_path + 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "file_names = ['node_information.csv', 'cleaned_titles.csv', 'cleaned_abstracts.csv']\n",
    "node_info = pd.read_csv(file_names[0], index_col=0, header=None)\n",
    "cleaned_titles = pd.read_csv(file_names[1], index_col=0, header=None)\n",
    "cleaned_abstracts = pd.read_csv(file_names[2], index_col=0, header=None)\n",
    "\n",
    "\n",
    "### TMP\n",
    "# - Overlap dos textos dos abstracts ok\n",
    "# - Overlap de keywords dos abstracts\n",
    "# - Número de pares de sinônimos entre as keywords\n",
    "# - Overlap entre títulos e abstracts opostos ok\n",
    "# - authors overlap ok\n",
    "# - mesmo journal ok\n",
    "# - tf-idf vectors cosine similarity\n",
    "### TMP\n",
    "\n",
    "# the columns of node_info are\n",
    "# (1) paper unique ID (integer)\n",
    "# (2) publication year (integer)\n",
    "# (3) paper title (string)\n",
    "# (4) authors (strings separated by ,)\n",
    "# (5) name of journal (optional) (string)\n",
    "# (6) abstract (string) - lowercased, free of punctuation except intra-word dashes\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __intersect_array(self, a, b, union=False):\n",
    "        array_a = []\n",
    "        for row in a:\n",
    "            if len(row)==0 or row[0]!=row[0] :\n",
    "                array_a.append([])\n",
    "            else:\n",
    "                array_a.append([s.strip() for s in row[0].split(',')])\n",
    "        # doc2\n",
    "        array_b = []\n",
    "        for row in b:\n",
    "            if len(row)==0 or row[0]!=row[0] :\n",
    "                array_b.append([])\n",
    "            else:\n",
    "                array_b.append([s.strip() for s in row[0].split(',')])\n",
    "\n",
    "        # check same size\n",
    "        assert len(array_a)==len(array_b)\n",
    "        intersec = [len(set(x).intersection(set(y))) for x, y in zip(array_a, array_b)]\n",
    "        if(not union):\n",
    "            return intersec\n",
    "        else:\n",
    "            uni = [len(x)+len(y)-inter for x,y,inter in zip(array_a, array_b, intersec)]\n",
    "            return intersec, uni\n",
    "\n",
    "    def __tfidf_cossine_similarity(self, a, b):\n",
    "        documents = np.concatenate((a,b), axis = 0)\n",
    "        newdoc = []\n",
    "        for item in documents:\n",
    "            if type(item[0]) == str:\n",
    "                newdoc.append(item[0].replace(\",\",\" \"))\n",
    "            else:\n",
    "                newdoc.append(\"\")\n",
    "        tfidf = (TfidfVectorizer().fit_transform(newdoc))\n",
    "\n",
    "        #print \"fitted\"\n",
    "        dot_products = []\n",
    "        n = len(a)\n",
    "        #print \"n = \", n\n",
    "        for i in range(n):\n",
    "            if i%10000==0:\n",
    "                print i\n",
    "            dot_products.append((tfidf[i]*tfidf[i+n].T).A[0] )\n",
    "        # normalizing\n",
    "        # dot_products = scale(dot_products)\n",
    "        #print \"gerou\"\n",
    "        return np.array(dot_products)            \n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ## build graph of the citation network\n",
    "        ids1 = X[:,0]\n",
    "        ids2 = X[:,1]\n",
    "        vertices = list(set(ids1.astype(str)).union(ids2.astype(str)))\n",
    "        edges = [tuple([str(row[0]), str(row[1])]) for row, link in zip(X,y) if link == 1]\n",
    "\n",
    "        self.graph = igraph.Graph() \n",
    "        self.graph.add_vertices(vertices)\n",
    "        self.graph.add_edges(edges)\n",
    "\n",
    "        self.di_network_graph = nx.DiGraph() \n",
    "        self.di_network_graph.add_nodes_from(vertices)\n",
    "        self.di_network_graph.add_edges_from(edges)\n",
    "\n",
    "        self.un_network_graph = nx.Graph() \n",
    "        self.un_network_graph.add_nodes_from(vertices)\n",
    "        self.un_network_graph.add_edges_from(edges)\n",
    "\n",
    "        vs = zip(vertices, range(len(vertices)))\n",
    "        self.hash_vs = {a:b for a,b in vs}\n",
    "        #print 'calculating shortest paths...'\n",
    "        #self.dmatrix = np.array(self.graph.shortest_paths())\n",
    "\n",
    "        #uncomment\n",
    "        # WARNING: cutoff should not be set in our final submission, which is equivalent to set it to infinity\n",
    "        print ('calculating betweenness centrality')\n",
    "        self.di_b_centrality = self.graph.betweenness(directed=True)\n",
    "        self.un_b_centrality = self.graph.betweenness(directed=False)\n",
    "\n",
    "        print ('calculating local edge connectivity')\n",
    "        H = build_auxiliary_edge_connectivity(self.di_network_graph)\n",
    "        R = build_residual_network(H, 'capacity')\n",
    "        self.di_connectivity = dict.fromkeys(self.di_network_graph, dict())\n",
    "        for u, v in itertools.combinations(self.di_network_graph, 2):\n",
    "            k = local_edge_connectivity(self.di_network_graph, u, v, auxiliary=H, residual=R)\n",
    "            self.di_connectivity[u][v] = k\n",
    "        H = build_auxiliary_edge_connectivity(self.un_network_graph)\n",
    "        R = build_residual_network(H, 'capacity')\n",
    "        self.un_connectivity = dict.fromkeys(self.un_network_graph, dict())\n",
    "        for u, v in itertools.combinations(self.un_network_graph, 2):\n",
    "            k = local_edge_connectivity(self.un_network_graph, u, v, auxiliary=H, residual=R)\n",
    "            self.un_connectivity[u][v] = k\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        print ('Transform...')\n",
    "\n",
    "        idx_docs1 = X[:,0]\n",
    "        idx_docs2 = X[:,1]\n",
    "        features = []\n",
    "\n",
    "        # difference in number of inlinks\n",
    "        print (\"difference in number of inlinks + number of times 'to' is cited\")\n",
    "        degrees_vs = self.graph.indegree()\n",
    "        degrees_idx1 = [degrees_vs[self.hash_vs[str(idx1)]] for idx1 in idx_docs1]\n",
    "        degrees_idx2 = [degrees_vs[self.hash_vs[str(idx2)]] for idx2 in idx_docs2]\n",
    "        diff_degrees = [a-b for a,b in zip(degrees_idx1,degrees_idx2)]\n",
    "        # degrees_idx2 = scale(degrees_idx2)\n",
    "        # diff_degrees = scale(diff_degrees)\n",
    "        degrees_idx2 = np.reshape(np.array(degrees_idx2), (-1,1))\n",
    "        diff_degrees = np.reshape(np.array(diff_degrees), (-1,1))\n",
    "        #degrees_idx1 = np.reshape(np.array(degrees_idx1), (-1, 1))\n",
    "        #degrees_idx2 = np.reshape(np.array(degrees_idx2), (-1, 1))\n",
    "        #print degrees_idx1\n",
    "        #features.append(degrees_idx1)\n",
    "\n",
    "        features.append(degrees_idx2)\n",
    "        features.append(diff_degrees)\n",
    "\n",
    "        # whether papers were classified in the same cluster or not\n",
    "        # TO IMPLEMENT YET\n",
    "\n",
    "\n",
    "        # uncomment\n",
    "        \n",
    "        print ('difference in betweeness centrality')\n",
    "        \n",
    "        di_b_centrality_idx1 = np.array([self.di_b_centrality[self.hash_vs[str(nodeid)]] for nodeid in idx_docs1])\n",
    "        di_b_centrality_idx2 = np.array([self.di_b_centrality[self.hash_vs[str(nodeid)]] for nodeid in idx_docs2])\n",
    "        diff_di_b_centrality = di_b_centrality_idx2 - di_b_centrality_idx1\n",
    "        # diff_di_b_centrality = scale(di_b_centrality)\n",
    "        diff_di_b_centrality = np.reshape(np.array(diff_di_b_centrality), (-1, 1))\n",
    "        \n",
    "        un_b_centrality_idx1 = np.array([self.un_b_centrality[self.hash_vs[str(nodeid)]] for nodeid in idx_docs1])\n",
    "        un_b_centrality_idx2 = np.array([self.un_b_centrality[self.hash_vs[str(nodeid)]] for nodeid in idx_docs2])\n",
    "        diff_un_b_centrality = un_b_centrality_idx2 - un_b_centrality_idx1\n",
    "        # diff_un_b_centrality = scale(diff_un_b_centrality)\n",
    "        diff_un_b_centrality = np.reshape(np.array(diff_un_b_centrality), (-1, 1))\n",
    "\n",
    "        features.append(diff_di_b_centrality)\n",
    "        features.append(diff_un_b_centrality)\n",
    "\n",
    "        # edge connectivity\n",
    "        di_connectivities = [self.di_connectivity[self.hash_vs[str(idx1)], self.di_connectivity[str(idx2)]] for idx1, idx2 in zip(idx_docs1, idx_docs2)]\n",
    "        un_connectivities = [self.un_connectivity[self.hash_vs[str(idx1)], self.un_connectivity[str(idx2)]] for idx1, idx2 in zip(idx_docs1, idx_docs2)]\n",
    "        di_connectivities = np.reshape(np.array(di_connectivities), (-1,1))\n",
    "        un_connectivities = np.reshape(np.array(un_connectivities), (-1,1))\n",
    "        features.append(di_connectivities)\n",
    "        features.append(un_connectivities)\n",
    "        \n",
    "\n",
    "\n",
    "        # shortest path \n",
    "        # uncomment\n",
    "        '''        \n",
    "        print 'shortest path...'\n",
    "        distances = [self.dmatrix[self.hash_vs[str(idx1)], self.hash_vs[str(idx2)]] for idx1, idx2 in zip(idx_docs1, idx_docs2)]\n",
    "        shortest_dist_3 = [1 if d <= 3 else 0 for d in distances]\n",
    "        shortest_dist_5 = [1 if d <= 5 and d > 3 else 0 for d in distances]\n",
    "        shortest_dist_10 = [1 if d <= 10 and d > 5 else 0 for d in distances]\n",
    "        shortest_dist_inf = [1 if d > 10 else 0 for d in distances]\n",
    "        shortest_dist_3 = np.reshape(np.array(shortest_dist_3), (-1,1))\n",
    "        shortest_dist_5 = np.reshape(np.array(shortest_dist_5), (-1,1))\n",
    "        shortest_dist_10 = np.reshape(np.array(shortest_dist_10), (-1,1))\n",
    "        shortest_dist_inf = np.reshape(np.array(shortest_dist_inf), (-1,1))\n",
    "        \n",
    "        distances = scale(distances)\n",
    "        distances = np.reshape(np.array(distances), (-1,1))\n",
    "        features.append(distances)\n",
    "        features.append(shortest_dist_3)\n",
    "        features.append(shortest_dist_5)\n",
    "        features.append(shortest_dist_10)\n",
    "        features.append(shortest_dist_inf)'''\n",
    "\n",
    "        print (\"we study neighbors\")\n",
    "        set_vs = set(self.graph.vs['name'])\n",
    "        is_in_graph1 = [(str(v) in set_vs) for v in idx_docs1]\n",
    "        is_in_graph2 = [(str(v) in set_vs) for v in idx_docs2]\n",
    "        neighbors1 = []\n",
    "        for node,is_in in zip(idx_docs1, is_in_graph1):\n",
    "            if is_in:\n",
    "                neighbors1.append(\",\".join([str(v) for v in self.graph.adjacent(str(node), mode='ALL')])) # both in and out neighbors\n",
    "            else:\n",
    "                neighbors1.append(\"\")\n",
    "        neighbors2 = []\n",
    "        for node,is_in in zip(idx_docs2, is_in_graph2):\n",
    "            if is_in:\n",
    "                neighbors2.append(\",\".join([str(v) for v in self.graph.adjacent(str(node), mode='ALL')])) # both in and out neighbors\n",
    "            else:\n",
    "                neighbors2.append(\"\")\n",
    "        #neighbors1 = [\",\".join([str(v) for v in self.graph.adjacent(int(node))]) if is_in else \"\" for node,is_in in zip(idx_docs1,is_in_graph1)]        \n",
    "        #neighbors2 = [\",\".join([str(v) for v in self.graph.adjacent(int(node))]) if is_in else \"\" for node,is_in in zip(idx_docs2,is_in_graph2)]        \n",
    "        intersec, uni = self.__intersect_array(neighbors1,neighbors2,union=True)\n",
    "        jaccard_coefficient = [float(a)/float(b) if b!=0 else 0 for a,b in zip(intersec,uni)]\n",
    "        #intersec = scale(intersec)\n",
    "        #jaccard_coefficient = scale(jaccard_coefficient)\n",
    "        intersec = np.reshape(np.array(intersec), (-1,1))\n",
    "        jaccard_coefficient = np.reshape(np.array(jaccard_coefficient), (-1,1))\n",
    "        features.append(intersec)\n",
    "        features.append(jaccard_coefficient)\n",
    "\n",
    "        # difference in number of inlinks\n",
    "\n",
    "\n",
    "        # cosine similarity of titles text\n",
    "        # uncomment\n",
    "        \n",
    "        print ('titles cosine similarity...')\n",
    "        titles_docs1 = np.array(cleaned_titles.loc[idx_docs1])\n",
    "        titles_docs2 = np.array(cleaned_titles.loc[idx_docs2])\n",
    "\n",
    "        features.append(self.__tfidf_cossine_similarity(titles_docs1, titles_docs2))                  \n",
    "\n",
    "        # cosine similarity of abstract text\n",
    "        print ('abstract cosine similarity...')\n",
    "        abstract_doc1 = np.array(cleaned_abstracts.loc[idx_docs1])\n",
    "        abstract_doc2 = np.array(cleaned_abstracts.loc[idx_docs2])\n",
    "        features.append(self.__tfidf_cossine_similarity(abstract_doc1, abstract_doc2))\n",
    "\n",
    "        # difference publication year\n",
    "        print ('difference publication year...')    \n",
    "        year_docs1 = np.array(node_info.loc[idx_docs1,1:1].astype(int))\n",
    "        year_docs2 = np.array(node_info.loc[idx_docs2,1:1].astype(int))\n",
    "        #features.append(year_docs1 - year_docs2)\n",
    "        diff_year = year_docs1 - year_docs2\n",
    "        # diff_year = scale(diff_year)\n",
    "        features.append(diff_year)\n",
    "        # titles overlap\n",
    "        print ('titles overlap...')\n",
    "        titles_docs1 = np.array(cleaned_titles.loc[idx_docs1])\n",
    "        titles_docs2 = np.array(cleaned_titles.loc[idx_docs2])\n",
    "        intersec = self.__intersect_array(titles_docs1, titles_docs2)\n",
    "        # intersec = scale(intersec)\n",
    "        intersec = np.reshape(np.array(intersec), (-1,1))\n",
    "        features.append(intersec)\n",
    "\n",
    "        # abstracts overlap\n",
    "        print ('abstracts overlap...')\n",
    "        abstract_doc1 = np.array(cleaned_abstracts.loc[idx_docs1])\n",
    "        abstract_doc2 = np.array(cleaned_abstracts.loc[idx_docs2])\n",
    "        intersec = self.__intersect_array(abstract_doc1, abstract_doc2)\n",
    "        # intersec = scale(intersec)\n",
    "        intersec = np.reshape(np.array(intersec), (-1,1))\n",
    "        features.append(intersec)\n",
    "\n",
    "        # titles and abstract overlap\n",
    "        print ('title and abstract overlap...')\n",
    "        intersec1 = np.array(self.__intersect_array(titles_docs1, abstract_doc2))\n",
    "        intersec2 = np.array(self.__intersect_array(abstract_doc1, titles_docs2))\n",
    "        intersec = np.reshape(intersec1 + intersec2, (-1,1))\n",
    "        # intersec = scale(intersec)\n",
    "        features.append(intersec)\n",
    "\n",
    "        # authors overlap\n",
    "        print ('authors overlap and self-citations')\n",
    "        authors_doc1 = np.array(node_info.loc[idx_docs1,3:3])\n",
    "        authors_doc2 = np.array(node_info.loc[idx_docs2,3:3])\n",
    "        intersec = self.__intersect_array(authors_doc1, authors_doc2)\n",
    "        self_citation = [1 if x>0 else 0 for x in intersec]\n",
    "        # intersec = scale(intersec)\n",
    "        intersec = np.reshape(np.array(intersec), (-1,1))\n",
    "        self_citation = np.reshape(np.array(self_citation), (-1,1))\n",
    "        features.append(intersec)\n",
    "        features.append(self_citation)\n",
    "\n",
    "        # same journal\n",
    "        print ('same journal...')\n",
    "        journal_doc1 = np.array(node_info.loc[idx_docs1, 4:4])\n",
    "        journal_doc2 = np.array(node_info.loc[idx_docs2, 4:4])\n",
    "        empties = np.array([\"\" for i in range(len(journal_doc1))])\n",
    "        unknown_journal = np.array((journal_doc1==\"\") | (journal_doc2==\"\")).astype(int)\n",
    "        is_same_journal = np.array((journal_doc1!=\"\") & (journal_doc2!=\"\") | (journal_doc1==journal_doc2)).astype(int)\n",
    "        is_other_journal = np.array((journal_doc1!=\"\") & (journal_doc2!=\"\") & (journal_doc1!=journal_doc2)).astype(int)\n",
    "        features.append(unknown_journal)\n",
    "        features.append(is_same_journal)\n",
    "        features.append(is_other_journal)\n",
    "\n",
    "        features_array = np.concatenate(tuple(features), axis=1)\n",
    "\n",
    "        return features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int) # read graph\n",
    "nodes = list(G.nodes())\n",
    "num_of_nodes = G.number_of_nodes()\n",
    "num_of_edges = G.number_of_edges()\n",
    "\n",
    "adjacency_matrix = nx.adjacency_matrix(G)\n",
    "\n",
    "\n",
    "abstracts_list = [] #Init List Abstracts\n",
    "with open('data/abstracts.txt', 'r', encoding = \"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        abstracts_list.append(line.split('|--|')[1].replace(\"\\n\", \"\"))  \n",
    "        \n",
    "authors_list = [] #Init List Authors\n",
    "with open('data/authors.txt', 'r', encoding = \"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        authors_list.append(set(line.split('|--|')[1].replace(\"\\n\", \"\").split(\",\"))) \n",
    "\n",
    "df = pd.DataFrame(data = {'abstracts': abstracts_list, 'authors': authors_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9f1f59101e07bffb7c2ecfaca1a3c7ffe3cd326ee75e914ab1b038684b38c5a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
