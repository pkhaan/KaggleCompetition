{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "\n",
    "from scipy import spatial\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def similarity(text_a, text_b):\n",
    "    model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return 1 - spatial.distance.cosine(get_vector(model, text_a), get_vector(model, text_b))\n",
    "\n",
    "def preprocess(s):\n",
    "    return [i.lower() for i in s]\n",
    "\n",
    "def get_vector(model, s):\n",
    "    print(model.wv.key_to_index())\n",
    "    return np.sum(np.array([model.wv[i] for i in preprocess(s)]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CommonNeighbors(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    return len(u_neighbors.intersection(v_neighbors))\n",
    "def common_neighbors(g, edges):\n",
    "    result = []\n",
    "    for edge in edges:\n",
    "        node_one, node_two = edge[0], edge[1]\n",
    "        num_common_neighbors = 0\n",
    "        try:\n",
    "            neighbors_one, neighbors_two = g.neighbors(node_one), g.neighbors(node_two)\n",
    "            for neighbor in neighbors_one:\n",
    "                if neighbor in neighbors_two:\n",
    "                    num_common_neighbors += 1\n",
    "            result.append((node_one, node_two, num_common_neighbors))\n",
    "        except:\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "    def AdamicAdar(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    aa = 0\n",
    "    for i in u_neighbors.intersection(v_neighbors):\n",
    "        aa += 1 / math.log(len(g.neighbors(i)))\n",
    "    return aa\n",
    "\n",
    "    def ResourceAllocation(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    ra = 0\n",
    "    for i in u_neighbors.intersection(v_neighbors):\n",
    "        ra += 1 / float(len(g.neighbors(i)))\n",
    "    return ra\n",
    "\n",
    "    \n",
    "    def JaccardCoefficent(u, v, g):\n",
    "    u_neighbors = set(g.neighbors(u))\n",
    "    v_neighbors = set(g.neighbors(v))\n",
    "    return len(u_neighbors.intersection(v_neighbors)) / float(len(u_neighbors.union(v_neighbors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AllFeatures(u,v,g1, g2):\n",
    "    '''\n",
    "    the change of features in two consecutive sub graphs\n",
    "    '''\n",
    "    try:\n",
    "        cn = CommonNeighbors(u, v, g2)\n",
    "        aa = AdamicAdar(u, v, g2)\n",
    "        ra = ResourceAllocation(u, v, g2)\n",
    "        jc = JaccardCoefficent(u, v, g2)\n",
    "        pa = PreferentialAttachment(u, v, g2)\n",
    "\n",
    "        delta_cn = cn - CommonNeighbors(u, v, g1)\n",
    "        delta_aa = aa - AdamicAdar(u, v, g1)\n",
    "        delta_ra = ra - ResourceAllocation(u, v, g1)\n",
    "        delta_jc = jc - JaccardCoefficent(u, v, g1)\n",
    "        delta_pa = pa - PreferentialAttachment(u, v, g1)\n",
    "        return {\"cn\":cn, \"aa\": aa, \"ra\":ra, \"jc\":jc, \"pa\":pa,\n",
    "            \"delta_cn\": delta_cn, \"delta_aa\": delta_aa, \"delta_ra\": delta_ra,\n",
    "             \"delta_jc\": delta_jc, \"delta_pa\": delta_pa}\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = [common_neighbors,\n",
    "                   nx.resource_allocation_index,\n",
    "                   nx.jaccard_coefficient,\n",
    "                   nx.adamic_adar_index,\n",
    "                   nx.preferential_attachment\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_fake_edge(g, neg_g,num_test_edges):\n",
    "    i = 0\n",
    "    while i < num_test_edges:\n",
    "        edge = random.sample(g.nodes(), 2)\n",
    "        try:\n",
    "            shortest_path = nx.shortest_path_length(g,source=edge[0],target=edge[1])\n",
    "            if shortest_path >= 2:\n",
    "                neg_g.add_edge(edge[0],edge[1], positive=\"False\")\n",
    "                i += 1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_from_file(filename):\n",
    "    print(\"----------------build graph--------------------\")\n",
    "    f = open(filename, \"rb\")\n",
    "    g = nx.read_edgelist(f)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_extraction(g, pos_num, neg_num, neg_mode, neg_distance=2, delete=1):\n",
    "    \"\"\"\n",
    "\n",
    "    :param g:  the graph\n",
    "    :param pos_num: the number of positive samples\n",
    "    :param neg_num: the number of negative samples\n",
    "    :param neg_distance: the distance between two nodes in negative samples\n",
    "    :param delete: if delete ==0, don't delete positive edges from graph\n",
    "    :return: pos_sample is a list of positive edges, neg_sample is a list of negative edges\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"----------------extract positive samples--------------------\")\n",
    "    # randomly select pos_num as test edges\n",
    "    pos_sample = random.sample(g.edges(), pos_num)\n",
    "    sample_g = nx.Graph()\n",
    "    sample_g.add_edges_from(pos_sample, positive=\"True\")\n",
    "    nx.write_edgelist(sample_g, \"sample_positive_\" +str(pos_num)+ \".txt\", data=['positive'])\n",
    "\n",
    "    # adding non-existing edges\n",
    "    print(\"----------------extract negative samples--------------------\")\n",
    "    i = 0\n",
    "    neg_g = nx.Graph()\n",
    "    produce_fake_edge(g,neg_g,neg_num)\n",
    "    nx.write_edgelist(neg_g, \"sample_negative_\" +str(neg_num)+ \".txt\", data=[\"positive\"])\n",
    "    neg_sample = neg_g.edges()\n",
    "    neg_g.add_edges_from(pos_sample,positive=\"True\")\n",
    "    nx.write_edgelist(neg_g, \"sample_combine_\" +str(pos_num + neg_num)+ \".txt\", data=[\"positive\"])\n",
    "\n",
    "    # remove the positive sample edges, the rest is the training set\n",
    "    if delete == 0:\n",
    "        return pos_sample, neg_sample\n",
    "    else:\n",
    "        g.remove_edges_from(pos_sample)\n",
    "        nx.write_edgelist(g, \"training.txt\", data=False)\n",
    "\n",
    "        return pos_sample, neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(g, pos_sample, neg_sample, feature_name, model=\"single\", combine_num=5):\n",
    "\n",
    "    data = []\n",
    "    if model == \"single\":\n",
    "        print (\"-----extract feature:\", feature_name.__name__, \"----------\")\n",
    "        preds = feature_name(g, pos_sample)\n",
    "        feature = [feature_name.__name__] + [i[2] for i in preds]\n",
    "        label = [\"label\"] + [\"Pos\" for i in range(len(feature))]\n",
    "        preds = feature_name(g, neg_sample)\n",
    "        feature1 = [i[2] for i in preds]\n",
    "        feature = feature + feature1\n",
    "        label = label + [\"Neg\" for i in range(len(feature1))]\n",
    "        data = [feature, label]\n",
    "        data = transpose(data)\n",
    "        print(\"----------write the feature to file---------------\")\n",
    "        write_data_to_file(data, \"features_\" + model + \"_\" + feature_name.__name__ + \".csv\")\n",
    "    else:\n",
    "        label = [\"label\"] + [\"1\" for i in range(len(pos_sample))] + [\"0\" for i in range(len(neg_sample))]\n",
    "        for j in feature_name:\n",
    "            print (\"-----extract feature:\", j.__name__, \"----------\")\n",
    "            preds = j(g, pos_sample)\n",
    "\n",
    "            feature = [j.__name__] + [i[2] for i in preds]\n",
    "            preds = j(g, neg_sample)\n",
    "            feature = feature + [i[2] for i in preds]\n",
    "            data.append(feature)\n",
    "\n",
    "        data.append(label)\n",
    "        data = transpose(data)\n",
    "        print(\"----------write the features to file---------------\")\n",
    "        write_data_to_file(data, \"features_\" + model + \"_\" + str(combine_num) + \".csv\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_data_to_file(data, filename):\n",
    "    csvfile = open(filename, \"w\")\n",
    "    writer = csv.writer(csvfile)\n",
    "    for i in data:\n",
    "        writer.writerow(i)\n",
    "    csvfile.close()\n",
    "\n",
    "\n",
    "def transpose(data):\n",
    "    return [list(i) for i in zip(*data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('data\\edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int) # read graph\n",
    "nodes = list(G.nodes())\n",
    "num_of_nodes = G.number_of_nodes()\n",
    "num_of_edges = G.number_of_edges()\n",
    "\n",
    "#adjacency_matrix = nx.adjacency_matrix(G)\n",
    "\n",
    "\n",
    "abstracts_list = [] #Init List Abstracts\n",
    "with open('data\\\\abstracts.txt', 'r', encoding = \"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        abstracts_list.append(set(line.split('|--|')[1].replace(\"\\n\", \"\").split()))  \n",
    "        \n",
    "authors_list = [] #Init List Authors\n",
    "with open('data\\\\authors.txt', 'r', encoding = \"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        authors_list.append(set(line.split('|--|')[1].replace(\"\\n\", \"\").split(\",\"))) \n",
    "\n",
    "df = pd.DataFrame(data = {'abstracts': abstracts_list, 'authors': authors_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare Data\n",
    "#### Baseline Data, Common Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "abstracts = df.to_dict()['abstracts']\n",
    "authors = df.to_dict()['authors']\n",
    "\n",
    "# Features:\n",
    "# (1) sum of number of unique terms of the two nodes' abstracts\n",
    "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
    "# (3) number of common terms between the abstracts of the two nodes\n",
    "# (4) number of common authors between the authorlists of the two nodes\n",
    "\n",
    "x = np.zeros((2*num_of_edges, 4))\n",
    "y = np.zeros(2*num_of_edges)\n",
    "\n",
    "for i, edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    x[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
    "    x[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
    "    x[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
    "    x[i,3] = len(authors[edge[0]].intersection(authors[edge[1]]))\n",
    "\n",
    "    y[i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, num_of_nodes-1)\n",
    "    n2 = randint(0, num_of_nodes-1)\n",
    "    x[num_of_edges+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
    "    x[num_of_edges+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
    "    x[num_of_edges+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
    "    x[num_of_edges+i,3] = len(authors[n1].intersection(authors[n2]))\n",
    "    \n",
    "    y[num_of_edges+i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Baseline Data, Common Authors, Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "abstracts = df.to_dict()['abstracts']\n",
    "authors = df.to_dict()['authors']\n",
    "H = G.to_directed()\n",
    "\n",
    "pagerank = nx.pagerank(H)\n",
    "\n",
    "# Features:\n",
    "# (1) sum of number of unique terms of the two nodes' abstracts\n",
    "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
    "# (3) number of common terms between the abstracts of the two nodes\n",
    "# (4) number of common authors between the author lists of the two nodes\n",
    "# (5) pagerank of first node\n",
    "# (6) pagerank of second node\n",
    "\n",
    "x = np.zeros((2*num_of_edges, 6))\n",
    "y = np.zeros(2*num_of_edges)\n",
    "\n",
    "for i, edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    x[i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
    "    x[i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
    "    x[i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
    "    x[i,3] = len(authors[edge[0]].intersection(authors[edge[1]]))\n",
    "    x[i,4] = pagerank[edge[0]]\n",
    "    x[i,5] = pagerank[edge[1]]\n",
    "\n",
    "    y[i] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, num_of_nodes-1)\n",
    "    n2 = randint(0, num_of_nodes-1)\n",
    "    x[num_of_edges+i,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
    "    x[num_of_edges+i,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
    "    x[num_of_edges+i,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
    "    x[num_of_edges+i,3] = len(authors[n1].intersection(authors[n2]))\n",
    "    x[num_of_edges+i,4] = pagerank[n1]\n",
    "    x[num_of_edges+i,5] = pagerank[n2]\n",
    "    \n",
    "    y[num_of_edges+i] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cross Validate Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = []\n",
    "train_accuracy = []\n",
    "cv_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "train_data = pd.DataFrame(data = {'sum_of_unique_terms': x_train[:,0], 'diff_of_unique_terms': x_train[:,1], 'common_terms': x_train[:,2], 'common_authors': x_train[:,3], 'target': y_train})\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=False)\n",
    "model = LogisticRegression(solver='liblinear',random_state=34)\n",
    "\n",
    "feature_df = train_data.drop('target' ,axis= 1)\n",
    "target_df = train_data[['target']]\n",
    "\n",
    "fold_index = 1\n",
    "for train_index, test_index in kf.split(train_data):\n",
    "    X_train_cv = feature_df.iloc[train_index]\n",
    "    X_test_cv = feature_df.iloc[test_index]\n",
    "    Y_train_cv = target_df.iloc[train_index]\n",
    "    Y_test_cv = target_df.loc[test_index]\n",
    "        \n",
    "    #Train the model\n",
    "    model.fit(X_train_cv, Y_train_cv.values.ravel()) #Training the model\n",
    "\n",
    "\n",
    "    train_pred = model.predict_proba(X_train_cv)\n",
    "    train_pred = train_pred[:,1]\n",
    "\n",
    "    train_pred_fixed = []\n",
    "    for item in train_pred: \n",
    "        if item >= 0.5: \n",
    "            train_pred_fixed.append(1)\n",
    "        else:\n",
    "            train_pred_fixed.append(0)\n",
    "\n",
    "    cv_pred = model.predict_proba(X_test_cv)\n",
    "    cv_pred = cv_pred[:,1]\n",
    "\n",
    "    cv_pred_fixed = []\n",
    "    for item in cv_pred: \n",
    "        if item >= 0.5: \n",
    "            cv_pred_fixed.append(1)\n",
    "        else:\n",
    "            cv_pred_fixed.append(0)\n",
    "\n",
    "    test_pred = model.predict_proba(x_test)\n",
    "    test_pred = test_pred[:,1]\n",
    "\n",
    "    test_pred_fixed = []\n",
    "    for item in test_pred: \n",
    "        if item >= 0.5: \n",
    "            test_pred_fixed.append(1)\n",
    "        else:\n",
    "            test_pred_fixed.append(0)\n",
    "\n",
    "    indexes.append(fold_index)\n",
    "    train_accuracy.append(accuracy_score(Y_train_cv, train_pred_fixed))\n",
    "    cv_accuracy.append(accuracy_score(Y_test_cv, cv_pred_fixed))\n",
    "    test_accuracy.append(accuracy_score(y_test, test_pred_fixed))\n",
    "\n",
    "    print(\"Current Index:\", fold_index)\n",
    "    fold_index += 1\n",
    "\n",
    "plt.plot(indexes, train_accuracy, label=\"Train Data\")\n",
    "plt.plot(indexes, cv_accuracy, label=\"Validate Data\")\n",
    "plt.plot(indexes, test_accuracy, label=\"Test Data\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Predict and Write to CSV\n",
    "\n",
    "#### Baseline Data, Common Authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 4 features as above\n",
    "X_test = np.zeros((len(node_pairs), 4))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = len(abstracts[node_pair[0]]) + len(abstracts[node_pair[1]])\n",
    "    X_test[i,1] = abs(len(abstracts[node_pair[0]]) - len(abstracts[node_pair[1]]))\n",
    "    X_test[i,2] = len(abstracts[node_pair[0]].intersection(abstracts[node_pair[1]]))\n",
    "    X_test[i,3] = len(authors[node_pair[0]].intersection(authors[node_pair[1]]))\n",
    "\n",
    "print('Size of test matrix:', X_test.shape)\n",
    "\n",
    "X_train, y_train = shuffle(x, y)\n",
    "\n",
    "\n",
    "# START OF MODEL\n",
    "\n",
    "\n",
    "# END OF MODEL\n",
    "\n",
    "print(\"Number of Predictions: \", len(y_pred))\n",
    "print('RMSLE:', np.sqrt(mean_squared_log_error(y_test, y_pred)))\n",
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"submission_common_authors_abc.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Baseline Data, Common Authors, Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test matrix: (106692, 6)\n",
      "Number of Predictions:  106692\n",
      "Predictions written to file\n"
     ]
    }
   ],
   "source": [
    "# Read test data. Each sample is a pair of nodes\n",
    "node_pairs = list()\n",
    "with open('data/test.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "\n",
    "# Create the test matrix. Use the same 6 features as above\n",
    "X_test = np.zeros((len(node_pairs), 6))\n",
    "for i,node_pair in enumerate(node_pairs):\n",
    "    X_test[i,0] = len(abstracts[node_pair[0]]) + len(abstracts[node_pair[1]])\n",
    "    X_test[i,1] = abs(len(abstracts[node_pair[0]]) - len(abstracts[node_pair[1]]))\n",
    "    X_test[i,2] = len(abstracts[node_pair[0]].intersection(abstracts[node_pair[1]]))\n",
    "    X_test[i,3] = len(authors[node_pair[0]].intersection(authors[node_pair[1]]))\n",
    "    X_test[i,4] = pagerank[node_pair[0]]\n",
    "    X_test[i,5] = pagerank[node_pair[1]]\n",
    "\n",
    "print('Size of test matrix:', X_test.shape)\n",
    "\n",
    "X_train, y_train = shuffle(x, y)\n",
    "\n",
    "\n",
    "# START OF MODEL\n",
    "clf = RandomForestClassifier(n_estimators= 20, max_depth=7, random_state=0, min_samples_leaf=20)\n",
    "model = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "# END OF MODEL\n",
    "\n",
    "print(\"Number of Predictions: \", len(y_pred))\n",
    "\n",
    "# Write predictions to a file\n",
    "predictions = zip(range(len(y_pred)), y_pred)\n",
    "with open(\"RandomForest_baseline_common_authors_pagerank_SECOND.csv\",\"w\") as pred:\n",
    "    csv_out = csv.writer(pred)\n",
    "    csv_out.writerow(['id','predicted'])\n",
    "    for row in predictions:\n",
    "        csv_out.writerow(row)\n",
    "\n",
    "print(\"Predictions written to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\peter\\Desktop\\GitHub\\KaggleCompetition\\pred.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/peter/Desktop/GitHub/KaggleCompetition/pred.ipynb#ch0000027?line=10'>11</a>\u001b[0m x_test\u001b[39m=\u001b[39mx[train_l:l,\u001b[39m0\u001b[39m:b\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/peter/Desktop/GitHub/KaggleCompetition/pred.ipynb#ch0000027?line=11'>12</a>\u001b[0m y_test\u001b[39m=\u001b[39mx[train_l:l,b\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/peter/Desktop/GitHub/KaggleCompetition/pred.ipynb#ch0000027?line=12'>13</a>\u001b[0m x_train \u001b[39m=\u001b[39m normalize(x_train, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, norm\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/peter/Desktop/GitHub/KaggleCompetition/pred.ipynb#ch0000027?line=13'>14</a>\u001b[0m x_test \u001b[39m=\u001b[39m normalize(x_test, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, norm\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/peter/Desktop/GitHub/KaggleCompetition/pred.ipynb#ch0000027?line=14'>15</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "l, b = x.shape\n",
    "train_l=int(0.75*l)\n",
    "x_train=x[0:train_l,0:b-1]\n",
    "y_train=x[0:train_l,b-1]\n",
    "x_test=x[train_l:l,0:b-1]\n",
    "y_test=x[train_l:l,b-1]\n",
    "x_train = normalize(x_train, axis=0, norm='max')\n",
    "x_test = normalize(x_test, axis=0, norm='max')\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(x_train)  \n",
    "x_train = scaler.transform(x_train)  \n",
    "x_test = scaler.transform(x_test)  \n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LogisticRegression(solver='liblinear',random_state=34)\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict_proba(x_test)\n",
    "y_pred = y_pred[:,1]\n",
    "\n",
    "y_predictions = []\n",
    "for i in y_pred: \n",
    "    if i >= 0.5: \n",
    "        y_predictions.append(1)\n",
    "    else:\n",
    "        y_predictions.append(0)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_predictions, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Baseline Data, Common Authors: 0.7265512529808893%\n",
    "##### Baseline Data, Common Authors, Pagerank: 0.727549461699922%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7774159398364037%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "l, b = x.shape\n",
    "train_l=int(0.75*l)\n",
    "x_train=x[0:train_l,0:b-1]\n",
    "y_train=x[0:train_l,b-1]\n",
    "x_test=x[train_l:l,0:b-1]\n",
    "y_test=x[train_l:l,b-1]\n",
    "\n",
    "\n",
    "x_train = min_max_scaler.fit_transform(np.array(x_train))\n",
    "x_test = min_max_scaler.fit_transform(np.array(x_test))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "clf = LinearRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "y_predictions = []\n",
    "for i in y_pred: \n",
    "    if i >= 0.5: \n",
    "        y_predictions.append(1)\n",
    "    else:\n",
    "        y_predictions.append(0)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_predictions, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Baseline Data, Common Authors: 0.7235657847019477%\n",
    "##### Baseline Data, Common Authors, Pagerank: 0.776417731117371%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7857093142947151%\n"
     ]
    }
   ],
   "source": [
    "x, y = shuffle(x, y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "# Use logistic regression to predict if two nodes are linked by an edge\n",
    "abc = AdaBoostClassifier(n_estimators=100, learning_rate=1)\n",
    "model = abc.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_predictions = []\n",
    "for i in y_pred: \n",
    "    if i >= 0.5: \n",
    "        y_predictions.append(1)\n",
    "    else:\n",
    "        y_predictions.append(0)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_predictions, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Baseline Data, Common Authors: 0.7058306378645294%\n",
    "##### Baseline Data, Common Authors, Pagerank: 0.7721501598965526% 0.7850297997355205%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ann Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.731879306492203%\n"
     ]
    }
   ],
   "source": [
    "x, y = shuffle(x, y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "clf = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(15,9), random_state=1)\n",
    "##start = datetime.datetime.now()\n",
    "model = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "y_predictions = []\n",
    "for i in y_pred: \n",
    "    if i >= 0.5: \n",
    "        y_predictions.append(1)\n",
    "    else:\n",
    "        y_predictions.append(0)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_predictions, y_test)) + \"%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 0. 0.]\n",
      "Accuracy: 0.7798720827579133%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# l, b = x.shape\n",
    "# train_l=int(0.75*l)\n",
    "# x_train=x[0:train_l,0:b-1]\n",
    "# y_train=x[0:train_l,b-1]\n",
    "# x_test=x[train_l:l,0:b-1]\n",
    "# y_test=x[train_l:l,b-1]\n",
    "\n",
    "\n",
    "# x_train = min_max_scaler.fit_transform(np.array(x_train))\n",
    "# x_test = min_max_scaler.fit_transform(np.array(x_test))\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators= 20, max_depth=7, random_state=0, min_samples_leaf=20)\n",
    "model = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_predictions, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbor CLASSIFIER Best Score Accuracy: 0.7841982643989318%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7694357648110364%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "##from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# l, b = x.shape\n",
    "# train_l=int(0.75*l)\n",
    "# x_train=x[0:train_l,0:b-1]\n",
    "# y_train=x[0:train_l,b-1]\n",
    "# x_test=x[train_l:l,0:b-1]\n",
    "# y_test=x[train_l:l,b-1]\n",
    "\n",
    "\n",
    "# x_train = min_max_scaler.fit_transform(np.array(x_train))\n",
    "# x_test = min_max_scaler.fit_transform(np.array(x_test))\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "#clf = svm.SVC()\n",
    "model = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_pred, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "##from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# l, b = x.shape\n",
    "# train_l=int(0.75*l)\n",
    "# x_train=x[0:train_l,0:b-1]\n",
    "# y_train=x[0:train_l,b-1]\n",
    "# x_test=x[train_l:l,0:b-1]\n",
    "# y_test=x[train_l:l,b-1]\n",
    "\n",
    "\n",
    "# x_train = min_max_scaler.fit_transform(np.array(x_train))\n",
    "# x_test = min_max_scaler.fit_transform(np.array(x_test))\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "clf = svm.SVC(random_state = 0, verbose = True)\n",
    "#clf = svm.SVC()\n",
    "model = clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "y_predictions = []\n",
    "for i in y_pred: \n",
    "    if i >= 0.5: \n",
    "        y_predictions.append(1)\n",
    "    else:\n",
    "        y_predictions.append(0)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_predictions, y_test)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word2Vec test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "abstracts_list_2 = []\n",
    "with open('data/abstracts.txt', 'r', encoding = \"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        abstracts_list_2.append(line.split('|--|')[1].replace(\"\\n\", \"\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 0 - Paper 1 0.8254383206367493\n",
      "Paper 1 - Paper 2 0.7354351282119751\n"
     ]
    }
   ],
   "source": [
    "# test_abstracts = [[word.lower() for word in abstract] for abstract in abstracts[:5]]\n",
    "\n",
    "tmp = abstracts_list_2[:4]\n",
    "\n",
    "fixed = []\n",
    "\n",
    "for abstract in tmp:\n",
    "    fixed.append([word.lower() for word in abstract])\n",
    "\n",
    "model = Word2Vec(fixed, min_count=1)\n",
    "\n",
    "def similarity(text_a, text_b):\n",
    "    return 1 - spatial.distance.cosine(get_vector(model, text_a), get_vector(model, text_b))\n",
    "\n",
    "def preprocess(s):\n",
    "    return [i.lower() for i in s]\n",
    "\n",
    "def get_vector(model, s):\n",
    "    return np.sum(np.array([model.wv[i] for i in preprocess(s)]), axis=0)\n",
    "\n",
    "print(\"Paper 0 - Paper 1\", similarity(fixed[0], fixed[1]))\n",
    "# print(\"Paper 0 - Paper 2\", similarity(tmp[0], tmp[2]))\n",
    "print(\"Paper 1 - Paper 2\", similarity(tmp[1], tmp[2]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9f1f59101e07bffb7c2ecfaca1a3c7ffe3cd326ee75e914ab1b038684b38c5a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
