{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31aaf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3356a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using= nx.DiGraph(), nodetype=int) # Create Directed Graph from Edgelist\n",
    "nodes = list(G.nodes())\n",
    "num_of_nodes = G.number_of_nodes()\n",
    "num_of_edges = G.number_of_edges()\n",
    "\n",
    "with open('data/abstracts_preprocessed.txt', 'r', encoding = \"UTF-8\") as f: # Read Abstracts\n",
    "    abstracts_list = [item.split(\",\") for item in f.read().split(\"#\")]\n",
    "        \n",
    "authors_list = []\n",
    "with open('data/authors.txt', 'r', encoding = \"UTF-8\") as f: # Read Author Lists\n",
    "    for line in f:\n",
    "        authors_list.append(set(line.split('|--|')[1].replace(\"\\n\", \"\").split(\",\"))) \n",
    "\n",
    "df = pd.DataFrame(data = {'abstracts': abstracts_list, 'authors': authors_list}) # Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607d0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(df.loc[:,\"abstracts\"], min_count=1) # Build Word2Vec Model\n",
    "\n",
    "def similarity(id1, id2): # Calculate Semantic Similarity of two Abstracts\n",
    "    text1 = list(set(df.loc[id1,\"abstracts\"])) # Get Abstract of Paper id1\n",
    "    text2 = list(set(df.loc[id2,\"abstracts\"])) # Get Abstract of Paper id2\n",
    "\n",
    "    vector1 = []\n",
    "    vector2 = []\n",
    "\n",
    "    for word in text1: # Collect Vectors of words in Abstract of Paper id1\n",
    "        vector1.append(w2v.wv[word])\n",
    "\n",
    "    mean1 = np.array(vector1).mean(axis=0)\n",
    "\n",
    "    for word in text2: # Collect Vectors of words in Abstract of Paper id1\n",
    "        vector2.append(w2v.wv[word])\n",
    "\n",
    "    mean2 = np.array(vector2).mean(axis=0)\n",
    "\n",
    "    return 1 - spatial.distance.cosine(mean1, mean2) # Return Cosine Distance of two Mean Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af17988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(A, B): # Calculate Jaccard Index of two Sets\n",
    "    nominator = A.intersection(B) # Get intersection of two sets\n",
    "\n",
    "    denominator = A.union(B) # Get union of two sets\n",
    "    \n",
    "    return len(nominator)/len(denominator) # Return ratio of sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9e1681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = nx.pagerank(G) # PageRank\n",
    "\n",
    "hits_vals = nx.hits(G) # HITS Algorithm\n",
    "\n",
    "hubness = hits_vals[0] # Hubness\n",
    "authority = hits_vals[1] # Authority\n",
    "\n",
    "cores = nx.core_number(G) # Core Number\n",
    "\n",
    "und_G = G.to_undirected() # Convert Graph to Undirected\n",
    "\n",
    "# Features:\n",
    "# (0) Semantic Similarity of Abstracts\n",
    "# (1) Jaccard Index of Abstracts\n",
    "# (2) Jaccard Index of Author Lists\n",
    "# (3) PageRank of First Node\n",
    "# (4) PageRank of Second Node\n",
    "# (5) Hubness of First Node\n",
    "# (6) Authority of Second Node\n",
    "# (7) Out-Degree of First Node\n",
    "# (8) In-Degree of Second Node\n",
    "# (9) Max k-Core of First Node\n",
    "# (10) Max k-Core of Second Node\n",
    "# (11) Number of Common Neighbors\n",
    "\n",
    "# (12) Class\n",
    "\n",
    "x_train = np.zeros((2*num_of_edges, 13))\n",
    "\n",
    "for i, edge in enumerate(G.edges()):\n",
    "    # an edge\n",
    "    x_train[i,0] = similarity(edge[0], edge[1])\n",
    "    \n",
    "    x_train[i,1] = jaccard_similarity(set(df.loc[edge[0],\"abstracts\"]), set(df.loc[edge[1],\"abstracts\"])) \n",
    "    x_train[i,2] = jaccard_similarity(set(df.loc[edge[0],\"authors\"]), set(df.loc[edge[1],\"authors\"]))\n",
    "    \n",
    "    x_train[i,3] = pagerank[edge[0]]\n",
    "    x_train[i,4] = pagerank[edge[1]]\n",
    "    \n",
    "    x_train[i,5] = hubness[edge[0]]\n",
    "    x_train[i,6] = authority[edge[1]]\n",
    "    \n",
    "    x_train[i,7] = G.out_degree[edge[0]]\n",
    "    x_train[i,8] = G.in_degree[edge[1]]\n",
    "    \n",
    "    x_train[i,9] = cores[edge[0]]\n",
    "    x_train[i,10] = cores[edge[1]]\n",
    "    \n",
    "    x_train[i,11] = len(list(nx.common_neighbors(und_G, edge[0], edge[1])))\n",
    "\n",
    "    x_train[i,12] = 1\n",
    "\n",
    "    # a randomly generated pair of nodes\n",
    "    n1 = randint(0, num_of_nodes-1)\n",
    "    n2 = randint(0, num_of_nodes-1)\n",
    "    \n",
    "    x_train[num_of_edges+i,0] = similarity(n1, n2)\n",
    "\n",
    "    x_train[num_of_edges+i,1] = jaccard_similarity(set(df.loc[n1,\"abstracts\"]), set(df.loc[n2,\"abstracts\"]))\n",
    "    x_train[num_of_edges+i,2] = jaccard_similarity(set(df.loc[n1,\"authors\"]), set(df.loc[n2,\"authors\"]))\n",
    "    \n",
    "    x_train[num_of_edges+i,3] = pagerank[n1]\n",
    "    x_train[num_of_edges+i,4] = pagerank[n2]\n",
    "    \n",
    "    x_train[num_of_edges+i,5] = hubness[n1]\n",
    "    x_train[num_of_edges+i,6] = authority[n2]\n",
    "    \n",
    "    x_train[num_of_edges+i,7] = G.out_degree[n1]\n",
    "    x_train[num_of_edges+i,8] = G.in_degree[n2]\n",
    "    \n",
    "    x_train[num_of_edges+i,9] = cores[n1]\n",
    "    x_train[num_of_edges+i,10] = cores[n2]\n",
    "    \n",
    "    x_train[num_of_edges+i,11] = len(list(nx.common_neighbors(und_G, n1, n2)))\n",
    "    \n",
    "    x_train[num_of_edges+i,12] = 0\n",
    "    \n",
    "with open(\"data/train.csv\",\"w\") as f: #NEEDS NAME\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(['Semantic Similarity of Abstracts','Jaccard Index of Abstracts', 'Jaccard Index of Author Lists', 'PageRank of First Node', 'PageRank of Second Node', 'Hubness of First Node', 'Authority of Second Node', 'Out-Degree of First Node', 'In-Degree of Second Node', 'Max k-Core of First Node', 'Max k-Core of Second Node', 'Number of Common Neighbors', 'Class'])\n",
    "    for row in x_train:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "557af0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape: (106692, 12)\n"
     ]
    }
   ],
   "source": [
    "node_pairs = []\n",
    "with open('data/test.txt', 'r') as f: # Read Test.txt\n",
    "    for line in f:\n",
    "        t = line.split(',')\n",
    "        node_pairs.append((int(t[0]), int(t[1])))\n",
    "        \n",
    "x_test = np.zeros((len(node_pairs), 12))\n",
    "\n",
    "# Create a Test matrix using the same 12 Features\n",
    "for i, node_pair in enumerate(node_pairs):\n",
    "    x_test[i,0] = similarity(node_pair[0], node_pair[1])\n",
    "    \n",
    "    x_test[i,1] = jaccard_similarity(set(df.loc[node_pair[0],\"abstracts\"]), set(df.loc[node_pair[1],\"abstracts\"]))\n",
    "    x_test[i,2] = jaccard_similarity(set(df.loc[node_pair[0],\"authors\"]), set(df.loc[node_pair[1],\"authors\"]))\n",
    "    \n",
    "    x_test[i,3] = pagerank[node_pair[0]]\n",
    "    x_test[i,4] = pagerank[node_pair[1]]\n",
    "    \n",
    "    x_test[i,5] = hubness[node_pair[0]]\n",
    "    x_test[i,6] = authority[node_pair[1]]\n",
    "    \n",
    "    x_test[i,7] = G.out_degree[node_pair[0]]\n",
    "    x_test[i,8] = G.in_degree[node_pair[1]]\n",
    "    \n",
    "    x_test[i,9] = cores[node_pair[0]]\n",
    "    x_test[i,10] = cores[node_pair[1]]\n",
    "    \n",
    "    x_test[i,11] = len(list(nx.common_neighbors(und_G, node_pair[0], node_pair[1])))\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "with open(\"data/test.csv\",\"w\") as f: # Writing to test.csv\n",
    "    csv_out = csv.writer(f)\n",
    "    csv_out.writerow(['Semantic Similarity of Abstracts','Jaccard Index of Abstracts', 'Jaccard Index of Author Lists', 'PageRank of First Node', 'PageRank of Second Node', 'Hubness of First Node', 'Authority of Second Node', 'Out-Degree of First Node', 'In-Degree of Second Node', 'Max k-Core of First Node', 'Max k-Core of Second Node', 'Number of Common Neighbors'])\n",
    "    for row in x_test:\n",
    "        csv_out.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
